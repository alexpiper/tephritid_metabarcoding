---
title: "iMapPESTS local metabarcoding workflow"
author: "A.M. Piper"
date: "`r Sys.Date()`"
output:
  
  html_document:
    highlighter: null
    theme: "flatly"
    code_download: true
    code_folding: show
    toc: true
    toc_float: 
      collapsed: false
      smooth_scroll: true
    df_print: paged    
  pdf_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
# Knitr global setup - change eval to true to run code
library(knitr)
knitr::opts_chunk$set(echo = TRUE, eval=FALSE, message=FALSE,error=FALSE, fig.show = "hold", fig.keep = "all")
opts_chunk$set(dev = 'png')
```

# Demultiplex sequencing reads
For this workflow to run, we need to first demultiplex the miseq run again as the miseq does not put indexes in fasta headers by default, and also obtain some necessary files from the sequencing folder. The below code is written for the Agriculture Victoria BASC server, and the locations will be different if you are using a different HPC cluster.

The output directory should be unique for each sequencing run, named as the flowcell id, within a directory called data

For example:

    root/
      ├── data/
         ├── CJL7D/

BASH:
```{bash demultiplex 1 mismatch}
#load module
module load bcl2fastq2/2.20.0-foss-2018b

#raise amount of available file handles
ulimit -n 4000

###Run1

#Set up input and outputs
inputdir=/group/sequencing/211119_M01054_0740_000000000-K3DVL #CHANGE TO YOUR SEQ RUN
outputdir=/group/pathogens/Alexp/Metabarcoding/tephritid_metabarcoding/data/K3DVL #CHANGE TO YOUR DATA FOLDER RUN
samplesheet=/group/pathogens/Alexp/Metabarcoding/tephritid_metabarcoding/data/K3DVL/SampleSheet_K3DVL.csv #CHANGE TO YOUR SAMPLESHEET

# convert samplesheet to unix format
dos2unix $samplesheet

#Demultiplex
bcl2fastq -p 12 --runfolder-dir $inputdir \
--output-dir $outputdir \
--sample-sheet $samplesheet \
--no-lane-splitting --barcode-mismatches 0

# Copy other necessary files and move fastqs
cd $outputdir
cp -r $inputdir/InterOp $outputdir
cp $inputdir/RunInfo.xml $outputdir
cp $inputdir/runInfo.xml $outputdir
cp $inputdir/runParameters.xml $outputdir
cp $inputdir/RunParameters.xml $outputdir
cp $samplesheet $outputdir
mv **/*.fastq.gz $outputdir

# Append fcid to start of sample names if missing
fcid=$(echo $inputdir | sed 's/^.*-//')
for i in *.fastq.gz; do
  if ! [[ $i == $fcid* ]]; then
  new=$(echo ${fcid} ${i}) #append together
  new=$(echo ${new// /_}) #remove any whitespace
  mv -v "$i" "$new"
  fi
done


###Run2

#Set up input and outputs
inputdir=/group/sequencing/220523_M03633_0591_000000000-K739J #CHANGE TO YOUR SEQ RUN
outputdir=/group/pathogens/Alexp/Metabarcoding/tephritid_metabarcoding/data/K739J #CHANGE TO YOUR DATA FOLDER RUN
samplesheet=/group/pathogens/Alexp/Metabarcoding/tephritid_metabarcoding/data/K739J/SampleSheet_K739J.csv #CHANGE TO YOUR SAMPLESHEET

# convert samplesheet to unix format
dos2unix $samplesheet

#Demultiplex
bcl2fastq -p 12 --runfolder-dir $inputdir \
--output-dir $outputdir \
--sample-sheet $samplesheet \
--no-lane-splitting --barcode-mismatches 0

# Copy other necessary files and move fastqs
cd $outputdir
cp -r $inputdir/InterOp $outputdir
cp $inputdir/RunInfo.xml $outputdir
cp $inputdir/runInfo.xml $outputdir
cp $inputdir/runParameters.xml $outputdir
cp $inputdir/RunParameters.xml $outputdir
cp $samplesheet $outputdir
mv **/*.fastq.gz $outputdir

# Append fcid to start of sample names if missing
fcid=$(echo $inputdir | sed 's/^.*-//')
for i in *.fastq.gz; do
  if ! [[ $i == $fcid* ]]; then
  new=$(echo ${fcid} ${i}) #append together
  new=$(echo ${new// /_}) #remove any whitespace
  mv -v "$i" "$new"
  fi
done

###Run3 

#Set up input and outputs
inputdir=/group/sequencing/220527_M01054_0780_000000000-K77JP #CHANGE TO YOUR SEQ RUN
outputdir=/group/pathogens/Alexp/Metabarcoding/tephritid_metabarcoding/data/K77JP #CHANGE TO YOUR DATA FOLDER RUN
samplesheet=/group/pathogens/Alexp/Metabarcoding/tephritid_metabarcoding/data/K77JP/SampleSheet_K77JP.csv #CHANGE TO YOUR SAMPLESHEET

# convert samplesheet to unix format
dos2unix $samplesheet

#Demultiplex
bcl2fastq -p 12 --runfolder-dir $inputdir \
--output-dir $outputdir \
--sample-sheet $samplesheet \
--no-lane-splitting --barcode-mismatches 0

# Copy other necessary files and move fastqs
cd $outputdir
cp -r $inputdir/InterOp $outputdir
cp $inputdir/RunInfo.xml $outputdir
cp $inputdir/runInfo.xml $outputdir
cp $inputdir/runParameters.xml $outputdir
cp $inputdir/RunParameters.xml $outputdir
cp $samplesheet $outputdir
mv **/*.fastq.gz $outputdir

# Append fcid to start of sample names if missing
fcid=$(echo $inputdir | sed 's/^.*-//')
for i in *.fastq.gz; do
  if ! [[ $i == $fcid* ]]; then
  new=$(echo ${fcid} ${i}) #append together
  new=$(echo ${new// /_}) #remove any whitespace
  mv -v "$i" "$new"
  fi
done
```

# Optional: Run R on BASC
You may wish to run this workflow through the BASC command line in order to take advantage of more processing power. To do this, you can start a new SLURM interactive session. Press the CODE button to the lower right to display the code for this optional step.

```{bash, class.source = 'fold-hide'}
cd /group/pathogens/Alexp/Metabarcoding/tephritid_metabarcoding
# Create new interactive SLURM session
sinteractive --ntasks=1 --cpus-per-task=10 --mem-per-cpu=10GB --time=72:00:00

module load R/4.1.0-foss-2021a
module load pkgconfig/1.5.1-GCCcore-9.3.0-Python-3.8.2
module load GDAL/3.3.0-foss-2021a
module load BLAST+/2.11.0-gompi-2020a
module load Pandoc/2.5

# Load R
R

# Run quit() to quit R once you are finished
```


# Install and load R packages and setup directories {.tabset}

The seqateurs R package also provides wrappers around other software packages for QC. For convenience we will download and install these software in a new folder called "bin"

```{r Manual install} 
#Set required packages
.cran_packages <- c(
  "devtools",
  "ggplot2",
  "gridExtra",
  "data.table",
  "tidyverse", 
  "stringdist",
  "patchwork",
  "vegan",
  "seqinr",
  "patchwork",
  "stringi",
  "phangorn",
  "magrittr",
  "galah"
  )

.bioc_packages <- c(
  "phyloseq",
  "DECIPHER",
  "Biostrings",
  "ShortRead",
  "ggtree",
  "savR",
  "dada2",
  "ngsReports"
  )

.inst <- .cran_packages %in% installed.packages()
if(any(!.inst)) {
   install.packages(.cran_packages[!.inst])
}
.inst <- .bioc_packages %in% installed.packages()
if(any(!.inst)) {
  if (!requireNamespace("BiocManager", quietly = TRUE)){
    install.packages("BiocManager")
  }
  BiocManager::install(.bioc_packages[!.inst], ask = F)
}

#Load all published packages
sapply(c(.cran_packages,.bioc_packages), require, character.only = TRUE)

# Install and load github packages
devtools::install_github("alexpiper/seqateurs", dependencies = TRUE)
library(seqateurs)

devtools::install_github("alexpiper/taxreturn", dependencies = TRUE)
library(taxreturn)

devtools::install_github("alexpiper/afdscraper", dependencies = TRUE)
library(afdscraper)

devtools::install_github("mikemc/speedyseq", dependencies = TRUE)
library(speedyseq)

#Install bbmap if its not in $path or in bin folder
if(Sys.which("bbduk") == "" & !file.exists("bin/bbmap/bbduk.sh")){
  seqateurs::bbmap_install(dest_dir = "bin")
}

#Install fastqc if its not in $path or in bin folder
if(Sys.which("fastqc") == "" & !file.exists("bin/FastQC/fastqc")){
  seqateurs::fastqc_install(dest_dir = "bin")
}

#Install BLAST if its not in $path or in bin folder
#if(Sys.which("blastn") == "" & (length(fs::dir_ls("bin", glob="*blastn.exe",recurse = TRUE)) ==0)){
#  taxreturn::blast_install(dest_dir = "bin")
#}



source("R/dependencies.R")
source("R/functions.R")
source("R/themes.R")
```

# Create sample sheet 

The directory structure should now look something like this:

    root/
    ├── data/
    │   ├── CJL7D/
    │   │  ├── R1.fastq.gz
    │   │  ├── R2.fastq.gz
    │   │  ├── runInfo.xml
    │   │  ├── runParameters.xml
    │   │  ├── SampleSheet.csv
    │   │  └── InterOp/
    │   └── fcid2/
    ├── sample_data/
    ├── reference
    ├── bin
    ├── output/
    └── doc/

The reference and bin folders can be copied from previous runs. 

In order to track samples and relevant QC statistics throughout the metabarcoding pipeline, we will first create a new samplesheet from our input samplesheets. This function requires both the SampleSheet.csv used for the sequencing run, and the runParameters.xml, both of which should have been automatically obtained from the demultiplexed sequencing run folder in the bash step above

```{r create samplesheet}
runs <- dir("data/") #Find all directories within data
SampleSheet <- list.files(paste0("data/", runs), pattern= "SampleSheet", full.names = TRUE)
runParameters <- list.files(paste0("data/", runs), pattern= "[Rr]unParameters.xml", full.names = TRUE)

# Create samplesheet containing samples and run parameters for all runs
samdf <- create_samplesheet(SampleSheet = SampleSheet, runParameters = runParameters, template = "V4") %>%
  distinct()

# Check if sampleids contain fcid, if not; attatch
samdf <- samdf %>%
  mutate(sample_id = case_when(
    !str_detect(sample_id, fcid) ~ paste0(fcid,"_",sample_id),
    TRUE ~ sample_id
  ))

# Check if samples match samplesheet
fastqFs <- purrr::map(list.dirs("data", recursive=FALSE),
                      list.files, pattern="_R1_", full.names = TRUE) %>%
  unlist() %>%
  str_remove(pattern = "^(.*)\\/") %>%
  str_remove(pattern = "(?:.(?!_S))+$")
fastqFs <- fastqFs[!str_detect(fastqFs, "Undetermined")]

#Check missing in samplesheet
if (length(setdiff(fastqFs, samdf$sample_id)) > 0) {warning("The fastq file/s: ", setdiff(fastqFs, samdf$sample_id), " are not in the sample sheet") }

#Check missing fastqs
if (length(setdiff(samdf$sample_id, fastqFs)) > 0) {
  warning(paste0("The fastq file: ",
                 setdiff(samdf$sample_id, fastqFs),
                 " is missing, dropping from samplesheet \n")) 
  samdf <- samdf %>%
    filter(!sample_id %in% setdiff(samdf$sample_id, fastqFs))
  logdf <- logdf %>%
    filter(!sample_id %in% setdiff(logdf$sample_id, fastqFs))
}

# Create demuxed samplesheet
samdf <- samdf %>%
  mutate(pcr_primers = case_when(
    str_detect(sample_id, "_fwh")  ~ "fwhF2-fwhR2nDac",
    str_detect(sample_id, "_EIF")  ~ "EIF3LminiF4-EIF3lminiR4",
    TRUE ~ "fwhF2-fwhR2nDac;EIF3LminiF4-EIF3lminiR4",
  ),
  for_primer_seq = case_when(
    str_detect(sample_id, "_fwh")  ~ "GGDACWGGWTGAACWGTWTAYCCHCC",
    str_detect(sample_id, "_EIF")  ~ "GATGCGYCGTTATGCYGATGC",
    TRUE ~ "GGDACWGGWTGAACWGTWTAYCCHCC;GATGCGYCGTTATGCYGATGC"
  ),
  rev_primer_seq = case_when(
    str_detect(sample_id, "_fwh")  ~ "GTRATWGCHCCIGCTAADACHGG",
    str_detect(sample_id, "_EIF")  ~ "TTRAAYACTTCYARATCRCC",
    TRUE ~ "GTRATWGCHCCIGCTAADACHGG;TTRAAYACTTCYARATCRCC"
  ))

# Params to add in step_add_parameters
params <- tibble(
  pcr_primers = c("fwhF2-fwhR2nDac", "EIF3LminiF4-EIF3lminiR4"),
  target_gene=c("COI", "EIF3L"),
  phmm = c("diagnostic_alignments/model/Bactrocera_COI.rds", "diagnostic_alignments/model/Bactrocera_EIF3L.rds"),
  ref_db = c("reference/COI_idtaxa.rds","reference/EIF3L_idtaxa.rds"),
  blast_db = c("reference/COI_hierarchial.fa", "reference/EIF3L_hierarchial.fa"),
  exp_length = c(205, 217),
  genetic_code = c("SGC4", "SGC0"),
  coding = c(TRUE, TRUE)
)

#Write out updated sample CSV for use
write_csv(samdf, "sample_data/Sample_info.csv")
write_csv(params, "sample_data/loci_params.csv")
```



# Quality checks:

We will conduct 3 quality checks. Firstly a check of the entire sequence run, followed by a sample level quality check to identify potential issues with specific samples. And then a calculation of the index switching rate by summarising correctly assigned vs missasigned indices.

```{r QC}
#Load sample sheet
samdf <- read.csv("sample_data/Sample_info.csv", stringsAsFactors = FALSE)

test <- samdf %>%
  group_by(pcr_primers) %>%
  #dplyr::slice(1:2) %>% #TESTIGN ONLY
  ungroup() %>%
   mutate(
     sample_qc = purrr::pmap(dplyr::select(., sample_id, fcid),
      .f = ~step_sample_qc(sample_id = ..1, fcid=..2, multithread=FALSE, quiet=TRUE)
   )) %>%
  group_by(fcid) %>%
  nest() %>%
  mutate(seq_qc = purrr::map(fcid, step_seq_qc),
         multi_qc = purrr::map(fcid, step_multiqc, quiet=FALSE),
         switching_qc = purrr::map(fcid, step_switching_calc, multithread=FALSE, quiet=TRUE)) %>%
  unnest(data)

```

# Trim Primers {.tabset}

DADA2 requires Non-biological nucleotides i.e. primers, adapters, linkers, etc to be removed. Following demultiplexing however primer sequences still remain in the reads and must be removed prior to use with the DADA2 algorithm. 
For this workflow we will be using the Kmer based adapter trimming software BBDuk (Part of BBTools package https://jgi.doe.gov/data-and-tools/bbtools/) to trim the primers from our raw data files. the seqateurs R package contains a wrapper fucntion to call bbduk from R to trim primers.

If multiple primers have been multiplexed per library, use the multiplexed primer option below, otherwise proceed with the regular single primer workflow.

## Primer trimming

```{R}

# Need to have option for length varaible markers and multipple primers
# For the trunclen parameter have an option for "estimate" which calls estimate_trucnlen or NULL

# Could still group by fcid for the per-sample, but then calculate within that.
# Perhaps a set_trunclen parameter that adds an extra column?

test2 <- samdf %>%
  filter(fcid== "K77JP") %>%
  dplyr::slice(1:3) %>%
  ungroup() %>%
  mutate(
   # Primer trimming
   primer_trim = purrr::pmap(dplyr::select(., sample_id, for_primer_seq, rev_primer_seq, pcr_primers, fcid),
   .f = ~step_primer_trim2(sample_id = ..1, for_primer_seq=..2, rev_primer_seq=..3, pcr_primers = ..4,
                         input_dir = paste0("data/",..5), output_dir =  paste0("data/",..5,"/01_trimmed"),
                         qc_dir=paste0("output/logs/",..5), quiet = FALSE)
   ),
   # Read filtering
    read_filter = purrr::pmap(dplyr::select(., sample_id, fcid),
    .f = ~step_read_filter(sample_id = ..1,
                          input_dir = paste0("data/",..2,"/01_trimmed/"), output_dir = paste0("data/",..2,"/02_filtered"),
                          maxEE = 1, truncLen = 150, rm.lowcomplex = 0,
                          quiet = FALSE)
   ))%>%
  mutate(
      # Pre-filtering quality plots
     prefilt_qualplots = purrr::pmap(dplyr::select(., sample_id, fcid),
     .f = ~plot_read_quals(sample_id = ..1,
                           input_dir = paste0("data/",..2,"/01_trimmed/"), truncLen=NULL, quiet = FALSE)
   ),
    # Post-filtering quality plots
    postfilt_qualplots = purrr::pmap(dplyr::select(., sample_id, fcid),
    .f = ~plot_read_quals(sample_id = ..1,
                          input_dir = paste0("data/",..2,"/02_filtered/"), truncLen=NULL, quiet = FALSE)
    ))

# Write out quality plots 
test2 %>%
  group_by(fcid) %>%
  nest() %>%
  purrr::pwalk(dplyr::select(.,  fcid, data),
              .f = ~{
    pdf(file=paste0("output/logs/",..1,"/prefilt_qualplots.pdf"), width = 11, height = 8 , paper="a4r")
        print(..2$prefilt_qualplots)
    try(dev.off(), silent=TRUE)
    pdf(file=paste0("output/logs/",..1,"/postfilt_qualplots.pdf"), width = 11, height = 8 , paper="a4r")
        print(..2$postfilt_qualplots)
    try(dev.off(), silent=TRUE)
    })

# Plot read numbers
gg.read_counts <- test2 %>%
  dplyr::select(sample_id, sample_name, primer_trim,fcid) %>%
  unnest(primer_trim) %>%
  mutate(primer_name = case_when(
    for_primer_seq == "GGDACWGGWTGAACWGTWTAYCCHCC" ~ "COI",
    for_primer_seq == "GATGCGYCGTTATGCYGATGC" ~ "EIF3L"
  ),
  group=case_when(
    str_detect(sample_id, "Diego") ~ "Diego",
    str_detect(sample_id, "Mock") ~ "Mock",
    str_detect(sample_id, "Trap") ~ "Trap",
    TRUE ~ "Trap"
  )) %>%
  ggplot(aes(x = sample_name, y = trimmed_output, fill=primer_name))+
  geom_col(position = "dodge")+
  facet_grid(fcid~group, drop=TRUE, scales="free_x", space="free_x") +
  base_theme + 
  theme(legend.position = "right")


```

# Infer sequence variants for each run {-}

This workflow uses the DADA2 algorithm to differentiate real sequences from error using their abundance and co-occurance patters. This relies on the assumption of a random error process where base errors are introduced randomly by either PCR polymerase or sequencing, real sequences will be high quality in the same way, while bad sequences are bad in different individual ways. DADA2 depends on a parameterized error model (the 16(possible bases) × 41(phred score) transition probabilities, for example, p(A→C, 35)), which is estimated from the data. DADA2’s default parameter estimation method is to perform a weighted loess fit to the regularized log of the observed mismatch rates as a function of their quality, separately for each transition type (for example, A→C mismatches are fit separately from A→G mismatches). Following error model learning, all identical sequencing reads are dereplicated into into “Amplicon sequence variants” (ASVs) with a corresponding abundance equal to the number of reads with that unique sequence. The forward and reverse reads are then merged together by aligning the denoised forward reads with the reverse-complement of the corresponding reverse reads, and then constructing the merged “contig” sequences. Following this step, a sequence variant table is constructed and saved as an RDS file.


```{r}
samdf <- read.csv("sample_data/Sample_info.csv", stringsAsFactors = FALSE)

# RUn DADA2
test3 <- samdf %>%
  group_by(fcid) %>%
  nest() %>%
  mutate(dada2 = purrr::pmap(dplyr::select(., fcid),
    .f = ~step_dada2(fcid = ..1,
                    input_dir = paste0("data/",..1,"/02_filtered"),
                    output = paste0("output/rds/",..1,"_seqtab.rds"),
                    qc_dir = paste0("output/logs/",..1),
                    quiet = FALSE)
   ))

#Error in `mutate()`:
#! Problem while computing `dada2 = purrr::pmap(...)`.
#x `dada2` must be size 1, not 3.
#i

```


# ASV filtering 

Following denoising and merging of reads, if there were multiple flowcells of data analyse the sequence tables from these will be merged together. Next the sequences are checked for chimeras, and all sequences containing stop codons are removed. The final cleaned sequence table is saved as output/rds/seqtab_final.rds

Note: this will change if you are using a coding marker or not

```{r chimera filt coding}

# Merge and subset seqtab and add a new column for the seqtab
# Then add another for the final filtered

# Add an stop point if all samples have remvoved

test4 <- samdf %>%
  ungroup() %>%
  group_by(target_gene) %>%
  nest() %>%
  mutate(subset_seqtab = purrr::map(target_gene, 
    .f = ~{
      seqtabs <- list.files("output/rds/", pattern="seqtab.rds", full.names = TRUE)
      if(length(seqtabs) > 1){
        st.all <- mergeSequenceTables(tables=seqtabs)
      } else if(length(seqtabs) == 1) {
        st.all <- readRDS(seqtabs)
      }
      st.all <- st.all[str_detect(rownames(st.all), .x),]
      st.all <- st.all[,colSums(st.all) > 0]
      return(st.all)
  })) %>%
  unnest(data) %>%
  group_by(target_gene, subset_seqtab, exp_length, phmm, coding, genetic_code, for_primer_seq, rev_primer_seq) %>%
  nest() %>%
  ungroup()%>%
  mutate(filtered_seqtab = purrr::pmap(dplyr::select(.,target_gene, subset_seqtab, exp_length, phmm, coding, genetic_code, for_primer_seq, rev_primer_seq),
    .f = ~step_filter_asvs(
      seqtab = ..2,
      output = paste0("output/rds/",..1,"_seqtab.cleaned.rds"),
      qc_dir = "output/logs/",
      min_length = ..3-10,
      max_length = ..3+10,
      phmm = ..4,
      check_frame = ..5,
      genetic_code = ..6,
      primers = c(..7, ..8),
      multithread = FALSE, 
      quiet = FALSE)
   )) %>%
  ungroup() %>%
  nest(data=everything()) %>%
  mutate(merged_seqtab = purrr::map(data, ~{
    seqtabs <- fs::dir_ls("output/rds/", glob="*.cleaned.rds")
    seqtabs <- seqtabs[str_detect(seqtabs, unique(.x$target_gene))]
      if(length(seqtabs) > 1){
        st.all <- mergeSequenceTables(tables=seqtabs)
      } else if(length(seqtabs) == 1) {
        st.all <- readRDS(seqtabs)
      }
    saveRDS(st.all, "output/rds/seqtab_final.rds")
    return(TRUE)
  })) %>%
  unnest(data) %>%
  unnest_wider(filtered_seqtab)


```

# Assign taxonomy  {.tabset}

Now that we have a cleaned table of sequences and their abundances across samples, we need to assign taxonomy to the sequences in order to identify taxa. The default approach is currently to use IDTAXA to assign heirarchial taxonomy, followed by a BLAST search for increased species level assignment. However there are a number of alternative classifiers you can use to do this, a few of which are represented in the tabs below.

## IDTAXA + BLAST

We will use the IDTAXA algorithm of Murali et al 2018 to assign taxonomy to the ASVs. IDTAXA requires a pre-trained classifier, which can be found in the reference folder, alternatively see the taxreturn r package if you wish to curate a reference database and train a new classifier.

To increase classification to species level, we will also incorporate a BLAST search. However as top hit assignment methods such as BLAST do not take the context of other sequences into account, to reduce the risk of over-classification we will only assign an ASV to species rank if the BLAST search agrees with IDTAXA at the Genus rank. 

```{r IDTAXA BLAST}
seqtab_final <- readRDS("output/rds/seqtab_final.rds")

saveRDS(test4, "test4.rds")
test4 <- readRDS("test4.rds")

test6 <- test4 %>%
  unnest(data) %>%
  ungroup() %>%
  group_by(target_gene, filtered_seqtab, ref_db, blast_db) %>%
  nest() %>%
  ungroup() %>%
  mutate(idtaxa = purrr::pmap(dplyr::select(.,target_gene, filtered_seqtab, ref_db),
    .f = ~step_assign_taxonomy(
      seqtab = ..2,
      database = ..3,
      ranks = c("Root","Kingdom", "Phylum","Class", "Order", "Family", "Genus","Species") ,
      output = paste0("output/rds/",..1,"_taxtab.rds"),
      qc_dir = "output/logs/",
      threshold = 60,
      multithread = FALSE, 
      quiet = FALSE)
   ))%>%
  mutate(blast = purrr::pmap(dplyr::select(.,target_gene, filtered_seqtab, blast_db),
    .f = ~step_blast_tophit(
      seqtab = ..2,
      database = ..3,
      ranks = c("Root","Kingdom", "Phylum","Class", "Order", "Family", "Genus","Species") ,
      output = paste0("output/rds/",..1,"_blast.rds"),
      qc_dir = "output/logs/",
      identity = 97,  
      coverage=95,
      evalue=1e06,
      max_target_seqs=5,
      max_hsp=5, 
      multithread = FALSE, 
      quiet = FALSE)
   ))%>%
  mutate(joint_tax = purrr::pmap(dplyr::select(.,target_gene, idtaxa, blast),
   .f = ~step_join_tax(
        tax = ..2,
        blast_spp = ..3,
        ranks = c("Root","Kingdom", "Phylum","Class", "Order", "Family", "Genus","Species") ,
        output = paste0("output/rds/",..1,"_taxblast.rds"),
        propagate_tax = TRUE)
     )) %>%
  ungroup() %>%
  nest(data=everything())%>%
  mutate(merged_tax = purrr::map(data,
    .f = ~{
      taxfiles <- list.files("output/rds/", pattern="_taxblast.rds", full.names = TRUE)
      taxfiles <- taxfiles[str_detect(taxfiles, unique(.x$target_gene))]

      tax <- taxfiles %>%
          purrr::map(readRDS) %>%
          bind_rows() %>%
        as.matrix()
      saveRDS(tax, "output/rds/final_tax.rds")
      return(tax)
  }))


# Need to have a function to subset to already found# Read in a-priori defined lists of identifyiable targets
  target_list <- readLines(paste0("reference/", db_name, "_id_list.txt")) %>%
    str_replace_all("_", " ")

# Read in final tax list
final_tax <- readRDS("output/rds/final_tax.rds")
  
# Only accept assignments that could be defined a-priori in the primer design

# Or should i merge the sequence names in the a-priori list 

# Or can i do a leave-one-out cross validation with the reference databases and these taxonomic assignment methods?

```

# Make phylogenetic tree

In addition to taxonomic assignment, we will create a phylogenetic tree from the identified sequences to allow interpetation within a phylognetic context.

```{r phylogeny}

# This should be done separately for each loci then merged, because phylogenies are only meant to be constructed with homologous loci

seqtab_final <- readRDS("output/rds/seqtab_final.rds")

seqs <- getSequences(seqtab_final)
names(seqs) <- seqs # This propagates to the tip labels of the tree
alignment <- AlignSeqs(DNAStringSet(seqs), anchor=NA)

library(phangorn)
phang.align <- phyDat(as(alignment, "matrix"), type="DNA")
dm <- dist.ml(phang.align)

#Fit NJ tree
treeNJ <- NJ(dm) # Note, tip order != sequence order
fit <- pml(treeNJ, data=phang.align)

#Fit ML tree
fitGTR <- update(fit, k=4, inv=0.2)
fitGTR <- optim.pml(fitGTR, model="GTR", optInv=TRUE, optGamma=TRUE,
                      rearrangement = "stochastic", control = pml.control(trace = 0))

# Write phytree to disk
saveRDS(fitGTR, "output/rds/phytree.rds") 

#Output newick tree
write.tree(fitGTR$tree, file="output/results/unfiltered/tree_unfiltered.nwk")
```


# Make Phyloseq object & Output final csvs

Finally, we will merge the sequence table, taxonomy table, phylogenetic tree, and sample data into a single phyloseq object, filter low abundance taxa, and output summary CSV files and fasta files of the identified taxa

```{r create PS, eval = FALSE}
seqtab_final <- readRDS("output/rds/seqtab_final.rds")

#Extract start of sequence names
rownames(seqtab_final) <- str_replace(rownames(seqtab_final), pattern="_S[0-9].*$", replacement="")

final_tax <- readRDS("output/rds/final_tax.rds") 
#phy <- readRDS("output/rds/phytree.rds")$tree
seqs <- DNAStringSet(colnames(seqtab_final))
names(seqs) <- seqs

#Load sample information
samdf <- read.csv("sample_data/Sample_info.csv", header=TRUE) %>%
  filter(!duplicated(sample_id)) %>%
  magrittr::set_rownames(.$sample_id) 

# Create phyloseq object
ps <- phyloseq(tax_table(final_tax),
               sample_data(samdf),
               otu_table(seqtab_final, taxa_are_rows = FALSE),
               #phy_tree(phy),
               refseq(seqs))

if(nrow(seqtab_final) > nrow(sample_data(ps))){
  message("Warning: the following samples were not included in phyloseq object, check sample names match the sample metadata")
  message(rownames(seqtab_final)[!rownames(seqtab_final) %in% sample_names(ps)])
}

saveRDS(ps, "output/rds/ps.rds") 

#Export raw csv
speedyseq::psmelt(ps) %>%
  filter(Abundance > 0) %>%
  dplyr::select(-Sample) %>%
  write_csv("output/results/unfiltered/raw_combined.csv")
  
#Summary export
seqateurs::summarise_taxa(ps, "Species", "sample_id") %>%
  spread(key="sample_id", value="totalRA") %>%
  write.csv(file = "output/results/unfiltered/spp_sum_unfiltered.csv")

seqateurs::summarise_taxa(ps, "Genus", "sample_id") %>%
  spread(key="sample_id", value="totalRA") %>%
  write.csv(file = "output/results/unfiltered/gen_sum_unfiltered.csv")

#Output fasta of all ASV's
seqateurs::ps_to_fasta(ps, out.file ="output/results/unfiltered/asvs_unfiltered.fasta", seqnames = "Species")
```

# Taxon & Sample filtering

Here we will remove all taxa that were not classified to Arthropoda, as these most likely represent residual erroneous sequences. This will be followed by removing all samples which are under a minimum read threshold. In this case, 1000.

```{R taxon filt}
#Set a threshold for minimum reads per sample
threshold <- 1000

ps0 <- ps %>%
  subset_taxa(
    Phylum == "Arthropoda"
  ) %>%
  filter_taxa(function(x) mean(x) > 0, TRUE) %>%
  prune_samples(sample_sums(.) >0, .) 

#Create rarefaction curve

rare <- otu_table(ps0) %>%
  as("matrix") %>%
  rarecurve(step=max(sample_sums(ps0))/100) %>%
  purrr::map(function(x){
    b <- as.data.frame(x)
    b <- data.frame(OTU = b[,1], count = rownames(b))
    b$count <- as.numeric(gsub("N", "",  b$count))
    return(b)
  }) %>%
  purrr::set_names(sample_names(ps0)) %>%
  bind_rows(.id="sample_id")

gg.rare <- ggplot(data = rare)+
  geom_line(aes(x = count, y = OTU, group=sample_id), alpha=0.5)+
  geom_point(data = rare %>% 
               group_by(sample_id) %>% 
               top_n(1, count),
             aes(x = count, y = OTU, colour=(count > threshold))) +
  geom_label(data = rare %>% 
               group_by(sample_id) %>% 
               top_n(1, count),
             aes(x = count, y = OTU,label=sample_id, colour=(count > threshold)),
             hjust=-0.05)+
  scale_x_continuous(labels =  scales::scientific_format()) +
  geom_vline(xintercept=threshold, linetype="dashed") +
  labs(colour = "Sample kept?") +
  xlab("Sequence reads") +
  ylab("Observed ASV's")

gg.rare

#Write out figure
pdf(file="fig/rarefaction.pdf", width = 11, height = 8 , paper="a4r")
  plot(gg.rare)
try(dev.off(), silent=TRUE)

#Remove all samples under the minimum read threshold 
ps1 <- ps0 %>%
  prune_samples(sample_sums(.)>=threshold, .) %>% 
  filter_taxa(function(x) mean(x) > 0, TRUE) #Drop missing taxa from table

#Message how many were removed
message(nsamples(ps) - nsamples(ps1), " Samples and ", ntaxa(ps) - ntaxa(ps1), " ASVs dropped")

# Export summary of filtered results
seqateurs::summarise_taxa(ps1, "Species", "sample_id") %>%
  spread(key="sample_id", value="totalRA") %>%
  write.csv(file = "output/results/filtered/spp_sum_filtered.csv")

seqateurs::summarise_taxa(ps1, "Genus", "sample_id") %>%
  spread(key="sample_id", value="totalRA") %>%
  write.csv(file = "output/results/filtered/gen_sum_filtered.csv")

#Output fasta of all ASV's
seqateurs::ps_to_fasta(ps1, "output/results/filtered/asvs_filtered.fasta", seqnames="Species")

#Output newick tree
write.tree(phy_tree(ps1), file="output/results/filtered/tree_filtered.nwk")

# output filtered phyloseq object
saveRDS(ps1, "output/rds/ps_filtered.rds") 
```


# Output final CSVs
We will output the final 3 filtered CSVs which will be uploaded to the imappests staging point database

* seqtab.csv
* taxtab.csv
* samdf.csv

```{R output csvs}
seqtab <- otu_table(ps1) %>%
  as("matrix") %>%
  as_tibble(rownames = "sample_id")

taxtab <- tax_table(ps1) %>%
  as("matrix") %>%
  as_tibble(rownames = "OTU") %>%
  unclassified_to_na(rownames = FALSE)

#Check taxonomy table outputs
if(!all(colnames(taxtab) == c("OTU", "Root", "Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species"))){
  message("Warning: Taxonomy table columns do not meet expectations for the staging database \n
          Database requires the columns: OTU, Root, Kingdom, Phylum, Class, Order, Family, Genus, Species ")
}

if(any(str_detect(taxtab$Species, "/"))){
  message("Warning: Taxonomy table contains taxa with clashes at the species level, these should be corrected before upload:")
  clashes <- taxtab$Species[str_detect(taxtab$Species, "/")]
  print(clashes[!is.na(clashes)])
}

samdf <- sample_data(ps1) %>%
  as("matrix") %>%
  as_tibble()

# Write out
write_csv(seqtab, "output/results/final/seqtab.csv")
write_csv(taxtab, "output/results/final/taxtab.csv")
write_csv(samdf, "output/results/final/samdf.csv")

#Write out combined
speedyseq::psmelt(ps1) %>%
  filter(Abundance > 0) %>%
  dplyr::select(-Sample) %>%
  write_csv("output/results/filtered/combined.csv")
```


# Optional - Check presence of taxa in Australia

This is an optional step to run an automated search against the Australian Faunal Directory ato see if the the detected species have been recorded in Australia before.

Press the CODE button to the lower right to display the code for this optional step.

```{r distribution check, class.source = 'fold-hide'}
ps1 <- readRDS("output/rds/ps_filtered.rds")

# Check presence on AFD
afd_check <- ps1 %>%
  speedyseq::psmelt() %>%
  dplyr::group_by(Family, Genus, Species) %>%
  summarise(metabarcoding_reads = sum(Abundance)) %>%
  filter(!str_detect(Species, "__")) %>%
  mutate(Species = Species %>% str_replace_all("_", " ")) %>%
  mutate(
    Family_present = afdscraper::check_afd_presence(Family),
    Genus_present = afdscraper::check_afd_presence(Genus),
    Species_present = afdscraper::check_afd_presence(Species)
  ) %>%
  dplyr::select(Family, Family_present, Genus, Genus_present, 
                Species, Species_present, metabarcoding_reads)
    
write_csv(afd_check, "output/results/final/afd_check.csv")

# Check presence on ALA
# First we need to set some data quality filters for ALA
# To view available filters, run: find_field_values("basis_of_record")
ala_quality_filter <- galah::select_filters(
      basisOfRecord = c("PreservedSpecimen", "LivingSpecimen",
                      "MaterialSample", "NomenclaturalChecklist"),
      profile = "ALA")

ala_quality_filter <- galah::select_filters(
      profile = "ALA")

ala_check <- ps1 %>%
  speedyseq::psmelt() %>%
  dplyr::group_by(Family, Genus, Species) %>%
  summarise(metabarcoding_reads = sum(Abundance)) %>%
  filter(!str_detect(Species, "__")) %>%
  mutate(Species = Species %>% str_replace_all("_", " ")) %>%
  mutate(
    species_present = purrr::map(Species, function(x){
    # first check name
    query <- select_taxa(x) %>% 
      as_tibble()%>%
      dplyr::filter(across(any_of("match_type"), ~!.x == "higherMatch"))
    # Then get occurance counts
    if(!is.null(query$scientific_name)){
      ala_occur <- ala_counts(taxa=query, filters=ala_quality_filter)
      return(data.frame(Species_present = ifelse(ala_occur > 0, TRUE, FALSE), ALA_counts = ala_occur))
    } else {
      return(data.frame(Species_present = FALSE, ALA_counts = 0))
    }
    })) %>%
  unnest(species_present) %>%
  dplyr::select(Family, Genus, Species, Species_present, ALA_counts, metabarcoding_reads)

write_csv(ala_check, "output/results/final/ala_check.csv")
```

# Output fate of reads through pipeline

```{r readtracker}
#Fraction of reads assigned to each taxonomic rank
sum_reads <- speedyseq::psmelt(ps) %>%
  gather("Rank","Name", rank_names(ps)) %>%
  group_by(Rank, sample_id) %>% 
  mutate(Name = replace(Name, str_detect(Name, "__"),NA)) %>% # This line turns the "__" we added to lower ranks back to NA's
  summarise(Reads_classified = sum(Abundance * !is.na(Name))) %>% 
  pivot_wider(names_from = "Rank",
              values_from = "Reads_classified") %>%
  dplyr::select(sample_id, rank_names(ps)) %>%
  dplyr::rename_at(rank_names(ps), ~paste0("reads_", .))

#Update log DF
logdf <- read_csv("output/logs/logdf.csv")

logdf <- logdf %>% 
  left_join(sum_reads,
  by=c("sample_id"))

write_csv(logdf, "output/logs/logdf.csv")


gg.all_reads <- logdf %>%
  dplyr::select(sample_id, fcid, starts_with("reads_"), -reads_total, -reads_pf) %>%
  pivot_longer(starts_with("reads_"),
               names_to = "type",
               values_to = "value") %>%
  group_by(fcid, type) %>% 
  summarise(value = sum(value, na.rm = TRUE)) %>%
  bind_rows(logdf %>%
  dplyr::select(fcid, reads_total, reads_pf) %>%
    distinct()%>%
  pivot_longer(starts_with("reads_"),
               names_to = "type",
               values_to = "value")
  ) %>%
  mutate(type = str_remove(type, "reads_")) %>%
  mutate(type = factor(type, levels = c(
  "total", "pf", "demulti",
  "trimmed", "qualfilt",
  "denoised", "merged",
  "chimerafilt", "sizefilt", "codonfilt",
  "Root", "Kingdom", "Phylum",
  "Class", "Order","Family",
  "Genus", "Species"))) %>% 
  ggplot(aes(x=type, y=value, fill=fcid)) +
  geom_bar(stat="identity") +
  facet_wrap(~fcid) +
  theme(axis.text.x = element_text(angle=90, hjust = 1, vjust=0.5)) 

gg.all_reads

pdf(paste0("output/logs/read_tracker_all.pdf"), width = 11, height = 8 , paper="a4r")
  gg.all_reads
try(dev.off(), silent=TRUE)
  
# Read tracker per sample
  
gg.separate_reads <- logdf %>%
  dplyr::select(sample_id, fcid, starts_with("reads_"), -reads_total, -reads_pf) %>%
  pivot_longer(starts_with("reads_"),
               names_to = "type",
               values_to = "value") %>%
  mutate(type = str_remove(type, "reads_")) %>%
  mutate(type = factor(type, levels = c(
  "total", "pf", "demulti",
  "trimmed", "qualfilt",
  "denoised", "merged",
  "chimerafilt", "sizefilt", "codonfilt",
  "Root", "Kingdom", "Phylum",
  "Class", "Order","Family",
  "Genus", "Species"))) %>% 
  ggplot(aes(x=type, y=value, fill=fcid)) +
  geom_bar(stat="identity") +
  facet_wrap(~sample_id) +
  theme(axis.text.x = element_text(angle=90, hjust = 1, vjust=0.5)) 

gg.separate_reads

pdf(paste0("output/logs/read_tracker_separate.pdf"), width = 11, height = 8 , paper="a4r")
  gg.separate_reads
try(dev.off(), silent=TRUE)
```


